{"paragraphs":[{"text":"%md\n# Introduction to Spark\n\nApache Spark is a unified computing engine and a set of libraries for parallel data processing on computer clusters. It supports multple widely used programming languages (Python, Java/Scala, R). Spark runs locally or ontop of a distribued storage cluster like Hadoop.During this module, we will walk you through the basics of using Spark as a parallel computing engine and some basic commands and operations Spark supports.","user":"innocent","dateUpdated":"2021-01-21T07:53:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Introduction to Spark</h1>\n<p>Apache Spark is a unified computing engine and a set of libraries for parallel data processing on computer clusters. It supports multple widely used programming languages (Python, Java/Scala, R). Spark runs locally or ontop of a distribued storage cluster like Hadoop.During this module, we will walk you through the basics of using Spark as a parallel computing engine and some basic commands and operations Spark supports.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215614981_-53478932","id":"20201119-171437_99598190","dateCreated":"2021-01-21T07:53:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:60"},{"text":"%md\n## 1. Architecture of a Spark Application\n\nIn a Spark application, the Cluster Manager (Standalone, YARN, Mesos) controls the physical machines and allocates resources to Spark apps. The main job of the cluster manager is to schedule jobs and manage resources for multiple Spark applications. The `Driver` process is in charge of executing commands across the executors.\n\n\nA SparkSession runs on a JVM - it is our entry point for running a Spark app. But Spark maintains language APIs to support interfacing with the JVM without the need to write explicit JVM instructions. \n\nThis notebook is automatically configured to create a Spark Sessions when it is run. When you run the code below, you should see the following:\n\n``` <pyspark.sql.session.SparkSession object at 0x...>```","user":"innocent","dateUpdated":"2021-01-21T07:53:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1. Architecture of a Spark Application</h2>\n<p>In a Spark application, the Cluster Manager (Standalone, YARN, Mesos) controls the physical machines and allocates resources to Spark apps. The main job of the cluster manager is to schedule jobs and manage resources for multiple Spark applications. The <code>Driver</code> process is in charge of executing commands across the executors.</p>\n<p>A SparkSession runs on a JVM - it is our entry point for running a Spark app. But Spark maintains language APIs to support interfacing with the JVM without the need to write explicit JVM instructions. </p>\n<p>This notebook is automatically configured to create a Spark Sessions when it is run. When you run the code below, you should see the following:</p>\n<p><code>&lt;pyspark.sql.session.SparkSession object at 0x...&gt;</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215614993_1675391830","id":"20201119-171903_1223654440","dateCreated":"2021-01-21T07:53:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"text":"%pyspark\nspark\n","user":"anum","dateUpdated":"2021-01-21T12:31:16+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"<pyspark.sql.session.SparkSession object at 0x7fbd65157310>\n"}]},"apps":[],"jobName":"paragraph_1611215614995_1579639029","id":"20201119-172956_336549600","dateCreated":"2021-01-21T07:53:34+0000","dateStarted":"2021-01-21T12:31:16+0000","dateFinished":"2021-01-21T12:31:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"text":"%md\n### 1.A DataFrames\nA `DataFrame` is the most common Structured API that Spark provides. It is a table of data with rows and columns. If you are familiar with Pandas, you can think of it as similar to the Pandas DataFrame API. The difference in Spark is that DataFrames can span thousands of computers, while a Pandas DataFrame sits on one computer. \n\n#### Partitions\nTo allow executors to work in parallel, Spark breaks up the data into chunks called *partitions*. A parition is a collection of rows that sits on one physical machine in a cluster. The partitions of a DataFrame represent how that data is broken up across the cluster.\n\n#### Transformations\nWhen we execute actions or operations in Spark, in most cases, we are manipulating partitions individually. This is what allows for the parallelization. In Spark, the data in each partition is *immutable*. When we run operations or commands on a DataFrame we are passing Spark a set of instructions of how to manipulate that DataFrame. For example:\n```\nsome_range = spark.range(1000).toDF(\"number\")\n\ndivis_by_two = some_range.where(\"number % 2 = 0\")\n```\nWhen we run this code, we will see no output. This is because, at its core, Spark is lazy. What we have just written is a abstract representation of a transformation we want to execute. Spark will not act on this instruction until we call an *action*. \n\n#### Lazy Evaluation\nSpark evaluates expressions lazily. This means that Spark will wait until the very last moment to execute the graph of computation instructions. Instead of modifying data immediately when you write it, Spark builds up a plan of transformations that you would like to apply to source data. Spark does this so that it can streamline the operations to a physical plan that will run as efficiently as possible across the cluster.\n\n#### Actions\nTransformation help us build a plan. In order to to trigger computation, we run an *action*. An action instructs Spark to compute a result from a series of transformations. A simple action is `count` which gives the total number of records in a `DataFrame`.\n\n``` divis_by_two.count()```\n\nGenerally there are three types of actions:\n1. Acions to view data in the console\n2. Actions to collect data to native objects in the respective language\n3. Actions to write to output data sources\n","user":"innocent","dateUpdated":"2021-01-21T12:00:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>1.A DataFrames</h3>\n<p>A <code>DataFrame</code> is the most common Structured API that Spark provides. It is a table of data with rows and columns. If you are familiar with Pandas, you can think of it as similar to the Pandas DataFrame API. The difference in Spark is that DataFrames can span thousands of computers, while a Pandas DataFrame sits on one computer. </p>\n<h4>Partitions</h4>\n<p>To allow executors to work in parallel, Spark breaks up the data into chunks called <em>partitions</em>. A parition is a collection of rows that sits on one physical machine in a cluster. The partitions of a DataFrame represent how that data is broken up across the cluster.</p>\n<h4>Transformations</h4>\n<p>When we execute actions or operations in Spark, in most cases, we are manipulating partitions individually. This is what allows for the parallelization. In Spark, the data in each partition is <em>immutable</em>. When we run operations or commands on a DataFrame we are passing Spark a set of instructions of how to manipulate that DataFrame. For example:</p>\n<pre><code>some_range = spark.range(1000).toDF(&quot;number&quot;)\n\ndivis_by_two = some_range.where(&quot;number % 2 = 0&quot;)\n</code></pre>\n<p>When we run this code, we will see no output. This is because, at its core, Spark is lazy. What we have just written is a abstract representation of a transformation we want to execute. Spark will not act on this instruction until we call an <em>action</em>. </p>\n<h4>Lazy Evaluation</h4>\n<p>Spark evaluates expressions lazily. This means that Spark will wait until the very last moment to execute the graph of computation instructions. Instead of modifying data immediately when you write it, Spark builds up a plan of transformations that you would like to apply to source data. Spark does this so that it can streamline the operations to a physical plan that will run as efficiently as possible across the cluster.</p>\n<h4>Actions</h4>\n<p>Transformation help us build a plan. In order to to trigger computation, we run an <em>action</em>. An action instructs Spark to compute a result from a series of transformations. A simple action is <code>count</code> which gives the total number of records in a <code>DataFrame</code>.</p>\n<p><code>divis_by_two.count()</code></p>\n<p>Generally there are three types of actions:<br/>1. Acions to view data in the console<br/>2. Actions to collect data to native objects in the respective language<br/>3. Actions to write to output data sources</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215614996_-912721701","id":"20201119-173624_1882080805","dateCreated":"2021-01-21T07:53:34+0000","dateStarted":"2021-01-21T12:00:43+0000","dateFinished":"2021-01-21T12:00:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"text":"%md\n### Analyzing Flights Data\n","user":"innocent","dateUpdated":"2021-01-21T07:53:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Analyzing Flights Data</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215614997_651793993","id":"20201119-173842_2061381563","dateCreated":"2021-01-21T07:53:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"text":"%md\n","user":"innocent","dateUpdated":"2021-01-21T07:53:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1611215614997_1297239446","id":"20210121-041311_214999086","dateCreated":"2021-01-21T07:53:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:65"},{"text":"%pyspark\nimport os\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom pyspark.sql import SQLContext\n\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nfrom pyspark.sql.functions import split, lower, when\nimport pyspark.sql.functions as F\nfrom pyspark import StorageLevel","user":"innocent","dateUpdated":"2021-01-21T12:03:59+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1611215614998_-1911671498","id":"20210120-165610_365043127","dateCreated":"2021-01-21T07:53:34+0000","dateStarted":"2021-01-21T12:03:59+0000","dateFinished":"2021-01-21T12:04:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"text":"%pyspark\n\nACCESS_KEY = os.environ['ACCESS_KEY']\nSECRET_KEY = os.environ['SECRET_KEY']\n\nsc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", ACCESS_KEY)\nsc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", SECRET_KEY)\n\nsq = SQLContext(sc)\n\npath_to_flights_data = \"s3n://sierra-leone-lake/training/data/demo/flight-data/csv/2015-summary.csv\"\npath_to_retail_data = \"s3n://sierra-leone-lake/training/data/demo/retail-data/all/*.csv\"","user":"innocent","dateUpdated":"2021-01-21T12:05:11+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1611215614999_1693548438","id":"20210120-164905_571774445","dateCreated":"2021-01-21T07:53:34+0000","dateStarted":"2021-01-21T12:04:59+0000","dateFinished":"2021-01-21T12:04:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:67"},{"text":"%pyspark\n#1. Read CSV Data\nflight_data_2015 = sq\\\n    .read\\\n    .option(\"inferSchema\", \"true\")\\\n    .option(\"header\", \"true\")\\\n    .csv(path_to_flights_data)\n    \n#2. Read out Spark's Internal execution plan\nflight_data_2015.createOrReplaceTempView(\"flight_data_2015\")\n\n\"\"\"\nBy default, when we execute an action that performs a shuffle (for example sorting data) Spark outputs the data into partitions. The default is 200 paritions. The following code changes that to 5. This code needs to be invoked at the head of the file.\n\n    ```spark.conf.set(\"spark.sdql.shuffle.partitions\", \"5\")```\n\"\"\"\n","user":"innocent","dateUpdated":"2021-01-21T12:06:26+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"'\\nBy default, when we execute an action that performs a shuffle (for example sorting data) Spark outputs the data into partitions. The default is 200 paritions. The following code changes that to 5. This code needs to be invoked at the head of the file.\\n\\n    ```spark.conf.set(\"spark.sdql.shuffle.partitions\", \"5\")```\\n'\n"}]},"apps":[],"jobName":"paragraph_1611215614999_1100173043","id":"20201123-221133_1799461241","dateCreated":"2021-01-21T07:53:34+0000","dateStarted":"2021-01-21T12:06:26+0000","dateFinished":"2021-01-21T12:06:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:68"},{"text":"%pyspark\nflight_data_2015.show()","user":"innocent","dateUpdated":"2021-01-21T12:08:56+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|   15|\n|       United States|            Croatia|    1|\n|       United States|            Ireland|  344|\n|               Egypt|      United States|   15|\n|       United States|              India|   62|\n|       United States|          Singapore|    1|\n|       United States|            Grenada|   62|\n|          Costa Rica|      United States|  588|\n|             Senegal|      United States|   40|\n|             Moldova|      United States|    1|\n|       United States|       Sint Maarten|  325|\n|       United States|   Marshall Islands|   39|\n|              Guyana|      United States|   64|\n|               Malta|      United States|    1|\n|            Anguilla|      United States|   41|\n|             Bolivia|      United States|   30|\n|       United States|           Paraguay|    6|\n|             Algeria|      United States|    4|\n|Turks and Caicos ...|      United States|  230|\n|       United States|          Gibraltar|    1|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1611230878686_445455173","id":"20210121-120758_473050345","dateCreated":"2021-01-21T12:07:58+0000","dateStarted":"2021-01-21T12:08:56+0000","dateFinished":"2021-01-21T12:08:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:69"},{"text":"%md \n#### Using SQL and DataFrames\nWhen writing out transformations and queries, Spark allows us to express them using the DataFrame API or as SQL queries. Below we demonstrate the same operation carried out with both approaches:\n","user":"innocent","dateUpdated":"2021-01-21T07:55:06+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Using SQL and DataFrames</h4>\n<p>When writing out transformations and queries, Spark allows us to express them using the DataFrame API or as SQL queries. Below we demonstrate the same operation carried out with both approaches:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615000_1403022286","id":"20201123-221147_1516379993","dateCreated":"2021-01-21T07:53:35+0000","dateStarted":"2021-01-21T07:55:06+0000","dateFinished":"2021-01-21T07:55:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:70"},{"text":"%pyspark\n#1. \nsql_approach = sq.sql(\"\"\"\n    SELECT DEST_COUNTRY_NAME, count(1)\n    FROM flight_data_2015\n    GROUP BY DEST_COUNTRY_NAME\n    \"\"\")\n    \ndataframe_approach = flight_data_2015\\\n    .groupBy(\"DEST_COUNTRY_NAME\")\\\n    .count()\n\nsql_approach.explain()\ndataframe_approach.explain()","user":"innocent","dateUpdated":"2021-01-21T12:10:34+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 200)\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3n://sierra-leone-lake/training/data/demo/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 200)\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3n://sierra-leone-lake/training/data/demo/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"}]},"apps":[],"jobName":"paragraph_1611215615001_1546475369","id":"20210120-165923_687313408","dateCreated":"2021-01-21T07:53:35+0000","dateStarted":"2021-01-21T12:10:34+0000","dateFinished":"2021-01-21T12:10:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:71"},{"text":"%pyspark\n#2. \nspark.sql(\"SELECT max(count) from flight_data_2015\").take(1), flight_data_2015.select(F.max(\"count\")).take(1)\n\n","user":"innocent","dateUpdated":"2021-01-21T12:11:51+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"([Row(max(count)=370002)], [Row(max(count)=370002)])\n"}]},"apps":[],"jobName":"paragraph_1611215615001_1822205147","id":"20201123-221902_1214116221","dateCreated":"2021-01-21T07:53:35+0000","dateStarted":"2021-01-21T12:11:51+0000","dateFinished":"2021-01-21T12:11:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:72"},{"text":"%pyspark\n# A More Complicated example. Lets find the top five destination countries in teh data.\n\nmax_sql = spark.sql(\"\"\"\n    SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n    FROM flight_data_2015\n    GROUP BY DEST_COUNTRY_NAME\n    ORDER BY sum(count) DESC\n    LIMIT 5\n    \"\"\")\n    \n\nmax_dataframe = flight_data_2015\\\n    .groupBy(\"DEST_COUNTRY_NAME\")\\\n    .sum(\"count\")\\\n    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n    .sort(F.desc(\"destination_total\"))\\\n    .limit(5)\n\nmax_sql.take(1), max_dataframe.take(1)","user":"innocent","dateUpdated":"2021-01-21T12:14:01+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"([Row(DEST_COUNTRY_NAME='United States', destination_total=411352)], [Row(DEST_COUNTRY_NAME='United States', destination_total=411352)])\n"}]},"apps":[],"jobName":"paragraph_1611215615002_-1033369469","id":"20201123-222543_769267885","dateCreated":"2021-01-21T07:53:35+0000","dateStarted":"2021-01-21T12:14:01+0000","dateFinished":"2021-01-21T12:14:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:73"},{"text":"%md\n### Basic Structured Operations\n\n1. Schemas\n2. Columns and Expressions\n3. Creating Dataframes\n4. Spark Types\n5. Column Operations\n  a. Adding Columns\n  b. Renaming Columns\n  c. Removing Columns\n  d. Changing Type\n6. Row Operations\n  a. Filtering Rows\n  b. Getting Unique Rows\n  c. Concatenating and Appending Rows\n  d. Sorting Rows\n  e. Limiting Rows\n7. Repartition and Coalesce\n8. Collecting to Driver","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Basic Structured Operations</h3>\n<ol>\n  <li>Schemas</li>\n  <li>Columns and Expressions</li>\n  <li>Creating Dataframes</li>\n  <li>Spark Types</li>\n  <li>Column Operations<br/> a. Adding Columns<br/> b. Renaming Columns<br/> c. Removing Columns<br/> d. Changing Type</li>\n  <li>Row Operations<br/> a. Filtering Rows<br/> b. Getting Unique Rows<br/> c. Concatenating and Appending Rows<br/> d. Sorting Rows<br/> e. Limiting Rows</li>\n  <li>Repartition and Coalesce</li>\n  <li>Collecting to Driver</li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615003_-525017948","id":"20201123-223130_1555831729","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74"},{"text":"%md\n#### **1. Columns, Rows, and Expressions**\n##### **Columns**\n\nIn Spark, columns are similar to columns in spreadsheets or in an R or Pandas dataframe. You can manipulate them with expressions. In Spark the easiest way to do this is to use Spark's built-in `col` or `column` functions:\n\n```\nfrom pyspark.sql.functions import col, column\n# with the columns function you pass the column name to get a reference back to the column\ncol(\"someColumnName\")\ncolumn(\"someColumnName\")\n```\n\nIf you want to perform transformations on a column, you must perform it on the column reference. You can use the `col` function or the `expr` function that Spark provides:\n```\n# Adds 5 to the reference value in column 'someCol'\ncol(\"someCol\") + 5\n\n# Same as above with an expression\nexpr(\"someCol - 5\")\n\n## Complex example\nThis: \n(((col(\"someCol\") + 5) * 200) - 6 < col(\"otherCol\")\n\nis the same as:\nexpr(\"(((someCol + 5) * 200) - 6) < otherCol\")\n\n\n```\n\n##### **Rows**\nYou can create a `Row` manually by create a Row object\n```python\nfrom pyspark.sql import Row\n\nmyRow = Row(\"Hello\", None, 1, False)\n\n# You can access the value as such\nmyRow[0] # \"Hello\"\n\n```\n\n","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4><strong>1. Columns, Rows, and Expressions</strong></h4>\n<h5><strong>Columns</strong></h5>\n<p>In Spark, columns are similar to columns in spreadsheets or in an R or Pandas dataframe. You can manipulate them with expressions. In Spark the easiest way to do this is to use Spark&rsquo;s built-in <code>col</code> or <code>column</code> functions:</p>\n<pre><code>from pyspark.sql.functions import col, column\n# with the columns function you pass the column name to get a reference back to the column\ncol(&quot;someColumnName&quot;)\ncolumn(&quot;someColumnName&quot;)\n</code></pre>\n<p>If you want to perform transformations on a column, you must perform it on the column reference. You can use the <code>col</code> function or the <code>expr</code> function that Spark provides:</p>\n<pre><code># Adds 5 to the reference value in column &#39;someCol&#39;\ncol(&quot;someCol&quot;) + 5\n\n# Same as above with an expression\nexpr(&quot;someCol - 5&quot;)\n\n## Complex example\nThis: \n(((col(&quot;someCol&quot;) + 5) * 200) - 6 &lt; col(&quot;otherCol&quot;)\n\nis the same as:\nexpr(&quot;(((someCol + 5) * 200) - 6) &lt; otherCol&quot;)\n\n\n</code></pre>\n<h5><strong>Rows</strong></h5>\n<p>You can create a <code>Row</code> manually by create a Row object</p>\n<pre><code class=\"python\">from pyspark.sql import Row\n\nmyRow = Row(&quot;Hello&quot;, None, 1, False)\n\n# You can access the value as such\nmyRow[0] # &quot;Hello&quot;\n\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615004_-1686787721","id":"20201123-223146_1515326300","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:75"},{"text":"%pyspark\nfrom pyspark.sql.functions import col, column, expr\n# with the columns function you pass the column name to get a reference back to the column\ncol(\"someColumnName\"), column(\"someColumnName\"), col(\"someCol\") + 5","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(Column<b'someColumnName'>, Column<b'someColumnName'>, Column<b'(someCol + 5)'>)\n"}]},"apps":[],"jobName":"paragraph_1611215615005_487243437","id":"20210120-170547_2123757921","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:76"},{"text":"%pyspark\n# Adds 5 to the reference value in column 'someCol'\na = col(\"someCol\") + 5\n\n# Same as above with an expression\nb = expr(\"someCol - 5\")\n\n## Complex example\n## This: \nc = ((col(\"someCol\") + 5) * 200) - 6 < col(\"otherCol\")\n\n## is the same as:\nd = expr(\"(((someCol + 5) * 200) - 6) < otherCol\")\n\nc, d","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(Column<b'((((someCol + 5) * 200) - 6) < otherCol)'>, Column<b'((((someCol + 5) * 200) - 6) < otherCol)'>)\n"}]},"apps":[],"jobName":"paragraph_1611215615005_1680598598","id":"20210120-170749_867641775","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:77"},{"text":"%pyspark\nfrom pyspark.sql import Row\n\nmyRow = Row(\"Hello\", None, 1, False)\n\n# You can access the value as such\nmyRow[0] # \"Hello\"","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"'Hello'\n"}]},"apps":[],"jobName":"paragraph_1611215615006_-1615968872","id":"20210120-171235_668435237","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:78"},{"text":"%md\n\n#### **2. Creating DataFrames**\n\nThe typical way of creating a DataFrame is from a raw data source. Below we demonstrate this reading a json file with a schema. \n\n```python\n# 1. Read the data\ndf = spark.read.format(\"json\").load(\"<path/to/json/flights>\")\n\n# 2. Create a temporary table to support SQL queries\ndf.createOrReplaceTempView(\"dfTable\")\n```\n\n##### **Schemas**\nIn order to manually create a DataFrame we need to provide both a schema and a collection of `Row`s.\n```\nfrom pyspark.sql import Row\nfrim pyspark.sql.types import StructField, StructType, StringType, LongType\n\nmySchema = StructType([\n    StructField(\"some\", StringType(), True),\n    StructField(\"col\", StringType(), True),\n    StructField(\"names\", LongType(), False)\n    ])\n    \nmyRow = Row(\"Hello\", None, 1)\nmyDf = spark.createDataFrame([myRow], myManualSchema)\nmyDf.show()\n```\n\nSpark provide us some other useful functions to write the equivalent of SQL queries on our DataFrames. Here are a couple Spark commands and their SQL equivalent:\n\nExample 1:\n```sql\nSELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2\n```\n```python\ndf.select(\"DEST_COUNTRY_NAME\").show(2)\n```\n\nExample 2:\n```sql\nSELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 2\n```\n```python\ndf.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAM\").show(2)\n```\n\nExample 3: Manipulating the name of a column with an alias\n```sql\nSELECT DEST_COUNTRY_NAME as destination FROM dfTable LIMIT 2\n```\n```python\ndf.select(expr(\"DEST_COUNTRY_NAME as destination\")).show(2)\n```\n\nExample 4: \n```sql\nSELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\nFROM dfTable\nLIMIT 2\n```\n```python\ndf.selectExpr(\n    \"*\",\n    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n    .show(2)\n```\n\n","user":"innocent","dateUpdated":"2021-01-21T12:19:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4><strong>2. Creating DataFrames</strong></h4>\n<p>The typical way of creating a DataFrame is from a raw data source. Below we demonstrate this reading a json file with a schema. </p>\n<pre><code class=\"python\"># 1. Read the data\ndf = spark.read.format(&quot;json&quot;).load(&quot;&lt;path/to/json/flights&gt;&quot;)\n\n# 2. Create a temporary table to support SQL queries\ndf.createOrReplaceTempView(&quot;dfTable&quot;)\n</code></pre>\n<h5><strong>Schemas</strong></h5>\n<p>In order to manually create a DataFrame we need to provide both a schema and a collection of <code>Row</code>s.</p>\n<pre><code>from pyspark.sql import Row\nfrim pyspark.sql.types import StructField, StructType, StringType, LongType\n\nmySchema = StructType([\n    StructField(&quot;some&quot;, StringType(), True),\n    StructField(&quot;col&quot;, StringType(), True),\n    StructField(&quot;names&quot;, LongType(), False)\n    ])\n    \nmyRow = Row(&quot;Hello&quot;, None, 1)\nmyDf = spark.createDataFrame([myRow], myManualSchema)\nmyDf.show()\n</code></pre>\n<p>Spark provide us some other useful functions to write the equivalent of SQL queries on our DataFrames. Here are a couple Spark commands and their SQL equivalent:</p>\n<p>Example 1:</p>\n<pre><code class=\"sql\">SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2\n</code></pre>\n<pre><code class=\"python\">df.select(&quot;DEST_COUNTRY_NAME&quot;).show(2)\n</code></pre>\n<p>Example 2:</p>\n<pre><code class=\"sql\">SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 2\n</code></pre>\n<pre><code class=\"python\">df.select(&quot;DEST_COUNTRY_NAME&quot;, &quot;ORIGIN_COUNTRY_NAM&quot;).show(2)\n</code></pre>\n<p>Example 3: Manipulating the name of a column with an alias</p>\n<pre><code class=\"sql\">SELECT DEST_COUNTRY_NAME as destination FROM dfTable LIMIT 2\n</code></pre>\n<pre><code class=\"python\">df.select(expr(&quot;DEST_COUNTRY_NAME as destination&quot;)).show(2)\n</code></pre>\n<p>Example 4: </p>\n<pre><code class=\"sql\">SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\nFROM dfTable\nLIMIT 2\n</code></pre>\n<pre><code class=\"python\">df.selectExpr(\n    &quot;*&quot;,\n    &quot;(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry&quot;)\\\n    .show(2)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615007_-365759357","id":"20201124-155021_60860148","dateCreated":"2021-01-21T07:53:35+0000","dateStarted":"2021-01-21T12:19:36+0000","dateFinished":"2021-01-21T12:19:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:79"},{"text":"%md\n#### **5. Column Operations**\n##### A. Adding Columns\nAdd a column named \"numberOne\" with 1's i all rows. `lit` is a built-in Spark function that allows us to specify a literal.\n```python\ndf.withColumn(\"numberOne\", lit(1)).show(2)\n```\nWe can also create new columns with *expressions*.\n```python\ndf.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME\")).show(2)\n```\n##### B. Renaming Columns\nWe can rename a column like this:\n\n```python\ndf.withColumn(\"numberOne\", expr(\"numberOne as NumberOne\")).show(2)\n```\nThough Spark has a built-in method for renaming, `withColumnRenamed`:\n```python\ndf.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\")\n```\n\n##### C. Removing Columns\nWe can remove columns by using `select` to choose the columns we want or you can use Spark's built-in `drop` method:\n```python\ndf.drop(\"ORIGIN_COUNTRY_NAME\")\n```\n","user":"innocent","dateUpdated":"2021-01-21T12:22:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4><strong>5. Column Operations</strong></h4>\n<h5>A. Adding Columns</h5>\n<p>Add a column named &ldquo;numberOne&rdquo; with 1&rsquo;s i all rows. <code>lit</code> is a built-in Spark function that allows us to specify a literal.</p>\n<pre><code class=\"python\">df.withColumn(&quot;numberOne&quot;, lit(1)).show(2)\n</code></pre>\n<p>We can also create new columns with <em>expressions</em>.</p>\n<pre><code class=\"python\">df.withColumn(&quot;withinCountry&quot;, expr(&quot;ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME&quot;)).show(2)\n</code></pre>\n<h5>B. Renaming Columns</h5>\n<p>We can rename a column like this:</p>\n<pre><code class=\"python\">df.withColumn(&quot;numberOne&quot;, expr(&quot;numberOne as NumberOne&quot;)).show(2)\n</code></pre>\n<p>Though Spark has a built-in method for renaming, <code>withColumnRenamed</code>:</p>\n<pre><code class=\"python\">df.withColumnRenamed(&quot;DEST_COUNTRY_NAME&quot;, &quot;dest&quot;)\n</code></pre>\n<h5>C. Removing Columns</h5>\n<p>We can remove columns by using <code>select</code> to choose the columns we want or you can use Spark&rsquo;s built-in <code>drop</code> method:</p>\n<pre><code class=\"python\">df.drop(&quot;ORIGIN_COUNTRY_NAME&quot;)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615008_1440213640","id":"20201124-160517_102688015","dateCreated":"2021-01-21T07:53:35+0000","dateStarted":"2021-01-21T12:22:12+0000","dateFinished":"2021-01-21T12:22:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:80"},{"text":"%md\n#### **6. Row Operations**\n##### A. Filtering Rows\nTo filter rows, we craete an expression that evaluates to true or false. You can filter out the rows with an expression that is equal to false. With a Dataframe you can do this with the `where` or `filter` methods.\n```python\ndf.filter(col(\"count\") < 2).show(2)\ndf.where(\"count < 2\").show(2)\n```\nYou can add multiple filters:\n```python\ndf.where(col(\"count\") < 2)\\\n  .where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\")\\\n  .show(2)\n\n```\n##### B. Unique Rows\n```python\ndf.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\".distinct().count()\n```\n##### C. Concatenating and Appending Rows\n```python\nfrom pyspark.sql import Row\n\nschema = df.schema\nnewRows = [\n    Row(\"New Country\", \"Other Country\", 5L),\n    Row(\"New Country 2\", \"Other Country 3\", 1L)\n]\n\nparallelizeRows = spark.sparkContext.parallelize(newRows)\nnewDF = spark.createDataFrame(parallelizeRow, schema)\n\ndf.union(newDF)\\\n    .where(\"count = 1\")\\\n    .where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n    .shwo()\n```\n\n##### D. Sorting Rows\n```python\ndf.sort(\"count\").show(5)\ndf.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n\n# To explicitly set the direction\nfrom pyspark.sql.functions import desc, asc\ndf.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)\n```\n","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4><strong>6. Row Operations</strong></h4>\n<h5>A. Filtering Rows</h5>\n<p>To filter rows, we craete an expression that evaluates to true or false. You can filter out the rows with an expression that is equal to false. With a Dataframe you can do this with the <code>where</code> or <code>filter</code> methods.</p>\n<pre><code class=\"python\">df.filter(col(&quot;count&quot;) &lt; 2).show(2)\ndf.where(&quot;count &lt; 2&quot;).show(2)\n</code></pre>\n<p>You can add multiple filters:</p>\n<pre><code class=\"python\">df.where(col(&quot;count&quot;) &lt; 2)\\\n  .where(col(&quot;ORIGIN_COUNTRY_NAME&quot;) != &quot;Croatia&quot;)\\\n  .show(2)\n\n</code></pre>\n<h5>B. Unique Rows</h5>\n<pre><code class=\"python\">df.select(&quot;ORIGIN_COUNTRY_NAME&quot;, &quot;DEST_COUNTRY_NAME&quot;.distinct().count()\n</code></pre>\n<h5>C. Concatenating and Appending Rows</h5>\n<pre><code class=\"python\">from pyspark.sql import Row\n\nschema = df.schema\nnewRows = [\n    Row(&quot;New Country&quot;, &quot;Other Country&quot;, 5L),\n    Row(&quot;New Country 2&quot;, &quot;Other Country 3&quot;, 1L)\n]\n\nparallelizeRows = spark.sparkContext.parallelize(newRows)\nnewDF = spark.createDataFrame(parallelizeRow, schema)\n\ndf.union(newDF)\\\n    .where(&quot;count = 1&quot;)\\\n    .where(col(&quot;ORIGIN_COUNTRY_NAME&quot;) != &quot;United States&quot;)\\\n    .shwo()\n</code></pre>\n<h5>D. Sorting Rows</h5>\n<pre><code class=\"python\">df.sort(&quot;count&quot;).show(5)\ndf.orderBy(&quot;count&quot;, &quot;DEST_COUNTRY_NAME&quot;).show(5)\n\n# To explicitly set the direction\nfrom pyspark.sql.functions import desc, asc\ndf.orderBy(col(&quot;count&quot;).desc(), col(&quot;DEST_COUNTRY_NAME&quot;).asc()).show(2)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615009_725029975","id":"20201125-195350_972779257","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:81"},{"text":"%md\n#### **7. Repartition and Coalesce**\n##### A. Repartition\nOftentimes it is better to instruct Spark to repatition data based on frequently filter columns. This is an important optimizations in particularly for large shuffles.\nGet the number of partitions:\n```python\ndf.rdd.getNumPartitions()\n```\nThen you could just specify the number of repartitions:\n```python\ndf.repartition(2)\n```\nOr the column to repartition by:\n```python\ndf.repartition(col(\"DEST_COUNTRY_NAME\"))\n```\nOr both the number and column:\n```python\ndf.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n```\n##### B. Coalesce\nAfter repartitioning and executing a transformation, we may want to combine partitions. We use `coalesce` to do this. It is oppositve of repartition. \n```python\ndf.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)\n```\n","user":"innocent","dateUpdated":"2021-01-21T12:26:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4><strong>7. Repartition and Coalesce</strong></h4>\n<h5>A. Repartition</h5>\n<p>Oftentimes it is better to instruct Spark to repatition data based on frequently filter columns. This is an important optimizations in particularly for large shuffles.<br/>Get the number of partitions:</p>\n<pre><code class=\"python\">df.rdd.getNumPartitions()\n</code></pre>\n<p>Then you could just specify the number of repartitions:</p>\n<pre><code class=\"python\">df.repartition(2)\n</code></pre>\n<p>Or the column to repartition by:</p>\n<pre><code class=\"python\">df.repartition(col(&quot;DEST_COUNTRY_NAME&quot;))\n</code></pre>\n<p>Or both the number and column:</p>\n<pre><code class=\"python\">df.repartition(5, col(&quot;DEST_COUNTRY_NAME&quot;))\n</code></pre>\n<h5>B. Coalesce</h5>\n<p>After repartitioning and executing a transformation, we may want to combine partitions. We use <code>coalesce</code> to do this. It is oppositve of repartition. </p>\n<pre><code class=\"python\">df.repartition(5, col(&quot;DEST_COUNTRY_NAME&quot;)).coalesce(2)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615009_-403114941","id":"20201125-201631_1738433392","dateCreated":"2021-01-21T07:53:35+0000","dateStarted":"2021-01-21T12:26:13+0000","dateFinished":"2021-01-21T12:26:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:82"},{"text":"%md\n#### **8. Collect to Driver**\nAs you work with data in Spark, you may want to collect some to the driver to manipulate it. There are several way to do this: `take`, `show`. `collect` is another that get all the data from the entire DataFrame.\n```python\ncollectDF = df.limit(10)\ncollectDF.take(5) # take works with an Integer count\ncollectDF.show() # this prints it out nicely\ncollectDF.show(5, False)\ncollectDF.collect()\n```\n","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4><strong>8. Collect to Driver</strong></h4>\n<p>As you work with data in Spark, you may want to collect some to the driver to manipulate it. There are several way to do this: <code>take</code>, <code>show</code>. <code>collect</code> is another that get all the data from the entire DataFrame.</p>\n<pre><code class=\"python\">collectDF = df.limit(10)\ncollectDF.take(5) # take works with an Integer count\ncollectDF.show() # this prints it out nicely\ncollectDF.show(5, False)\ncollectDF.collect()\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615010_-1022056604","id":"20201125-203000_2138622592","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:83"},{"text":"%md\n### **Working with Dates and Timestamps**","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3><strong>Working with Dates and Timestamps</strong></h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615011_1905393179","id":"20201125-203213_406685665","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:84"},{"text":"%md\nIn general, dates and time are constant challenges. It is necessary to keep track of timezones and to ensure that formats are correct and valid. Spark attempts to read dates or times on a best-effort basis. Spark provides a library of functions to work with dates and times. For examples, lets get the current date and current timestamps:\n```python\nfrom pyspark.sql.functions import current_date, current_timestamp\ndate_df = spark.range(10)\\\n    .withColumn(\"today\", current_date())\\\n    .withColumn(\"now\", current_timestamp())\n\ndate_df.createOrReplaceTempView(\"dateTable\")\n\ndate_df.printSchema()\n```\nOkay, now that we have a Dataframe with the current date and time stamp, lets add and subtract five days from today.\n```python\nfrom pyspark.sql.functions import date_add, date_sub\ndate_df.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)\n```\n","user":"innocent","dateUpdated":"2021-01-21T12:26:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In general, dates and time are constant challenges. It is necessary to keep track of timezones and to ensure that formats are correct and valid. Spark attempts to read dates or times on a best-effort basis. Spark provides a library of functions to work with dates and times. For examples, lets get the current date and current timestamps:</p>\n<pre><code class=\"python\">from pyspark.sql.functions import current_date, current_timestamp\ndate_df = spark.range(10)\\\n    .withColumn(&quot;today&quot;, current_date())\\\n    .withColumn(&quot;now&quot;, current_timestamp())\n\ndate_df.createOrReplaceTempView(&quot;dateTable&quot;)\n\ndate_df.printSchema()\n</code></pre>\n<p>Okay, now that we have a Dataframe with the current date and time stamp, lets add and subtract five days from today.</p>\n<pre><code class=\"python\">from pyspark.sql.functions import date_add, date_sub\ndate_df.select(date_sub(col(&quot;today&quot;), 5), date_add(col(&quot;today&quot;), 5)).show(1)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615011_1210610055","id":"20210120-183559_1587114030","dateCreated":"2021-01-21T07:53:35+0000","dateStarted":"2021-01-21T12:26:19+0000","dateFinished":"2021-01-21T12:26:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import current_date, current_timestamp, date_add, date_sub\n\ndate_df = spark.range(10)\\\n    .withColumn(\"today\", current_date())\\\n    .withColumn(\"now\", current_timestamp())\n\ndate_df.createOrReplaceTempView(\"dateTable\")\n\ndate_df.printSchema()\n\ndate_df.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)\n\n\n","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- id: long (nullable = false)\n |-- today: date (nullable = false)\n |-- now: timestamp (nullable = false)\n\n+------------------+------------------+\n|date_sub(today, 5)|date_add(today, 5)|\n+------------------+------------------+\n|        2021-01-15|        2021-01-25|\n+------------------+------------------+\nonly showing top 1 row\n\n"}]},"apps":[],"jobName":"paragraph_1611215615012_1061524325","id":"20210120-183610_76378962","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:86"},{"text":"%md\n\nAnother common operation is to find the difference between two dates.\n```python\nfrom pyspark.sql.functions import datediff, months_between, to_date\ndate_df.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n.select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n\ndateDF.select(\n    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n    to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n    .select(months_between(col(\"start\"), col(\"end\"))).show(1)\n```\n\nWe can use the `to_date` function to convert a string to a date, optionally with a specified format. \n```python\nfrom pyspark.sql.functions import to_date, lit\nspark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n    .select(to_date(col(\"date\"))).show(1)\n```\n\nWe can be more explicit with `to_date` and `to_timestamp`.\n```python\nfrom pyspark.sql.functions import to_date, to_timestamp\ndateFormat = \"yyyy-dd-MM\"\ncleanDateDF = spark.range(1).select(\n    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n\ncleanDateDF.createOrReplaceTempView(\"dateTable2\")\n\ncleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()\n\n```","user":"innocent","dateUpdated":"2021-01-21T12:27:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Another common operation is to find the difference between two dates.</p>\n<pre><code class=\"python\">from pyspark.sql.functions import datediff, months_between, to_date\ndate_df.withColumn(&quot;week_ago&quot;, date_sub(col(&quot;today&quot;), 7))\\\n.select(datediff(col(&quot;week_ago&quot;), col(&quot;today&quot;))).show(1)\n\ndateDF.select(\n    to_date(lit(&quot;2016-01-01&quot;)).alias(&quot;start&quot;),\n    to_date(lit(&quot;2017-05-22&quot;)).alias(&quot;end&quot;))\\\n    .select(months_between(col(&quot;start&quot;), col(&quot;end&quot;))).show(1)\n</code></pre>\n<p>We can use the <code>to_date</code> function to convert a string to a date, optionally with a specified format. </p>\n<pre><code class=\"python\">from pyspark.sql.functions import to_date, lit\nspark.range(5).withColumn(&quot;date&quot;, lit(&quot;2017-01-01&quot;))\\\n    .select(to_date(col(&quot;date&quot;))).show(1)\n</code></pre>\n<p>We can be more explicit with <code>to_date</code> and <code>to_timestamp</code>.</p>\n<pre><code class=\"python\">from pyspark.sql.functions import to_date, to_timestamp\ndateFormat = &quot;yyyy-dd-MM&quot;\ncleanDateDF = spark.range(1).select(\n    to_date(lit(&quot;2017-12-11&quot;), dateFormat).alias(&quot;date&quot;),\n    to_date(lit(&quot;2017-20-12&quot;), dateFormat).alias(&quot;date2&quot;))\n\ncleanDateDF.createOrReplaceTempView(&quot;dateTable2&quot;)\n\ncleanDateDF.select(to_timestamp(col(&quot;date&quot;), dateFormat)).show()\n\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615013_-216868696","id":"20201201-212239_1301911810","dateCreated":"2021-01-21T07:53:35+0000","dateStarted":"2021-01-21T12:27:33+0000","dateFinished":"2021-01-21T12:27:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:87"},{"text":"%pyspark\nfrom pyspark.sql.functions import datediff, months_between, to_date, lit, to_timestamp\n\ndate_df.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n.select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n\ndate_df.select(\n    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n    to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n    .select(months_between(col(\"start\"), col(\"end\"))).show(1)\n    \n    \nsq.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n    .select(to_date(col(\"date\"))).show(1)\n    \ndate_format = \"yyyy-dd-MM\"\nclean_date_df = sq.range(1).select(\n    to_date(lit(\"2017-12-11\"), date_format).alias(\"date\"),\n    to_date(lit(\"2017-20-12\"), date_format).alias(\"date2\"))\n\nclean_date_df.createOrReplaceTempView(\"dateTable2\")\n\nclean_date_df.select(to_timestamp(col(\"date\"), date_format)).show()","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------------+\n|datediff(week_ago, today)|\n+-------------------------+\n|                       -7|\n+-------------------------+\nonly showing top 1 row\n\n+--------------------------------+\n|months_between(start, end, true)|\n+--------------------------------+\n|                    -16.67741935|\n+--------------------------------+\nonly showing top 1 row\n\n+---------------+\n|to_date(`date`)|\n+---------------+\n|     2017-01-01|\n+---------------+\nonly showing top 1 row\n\n+----------------------------------+\n|to_timestamp(`date`, 'yyyy-dd-MM')|\n+----------------------------------+\n|               2017-11-12 00:00:00|\n+----------------------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1611215615013_1772239998","id":"20210120-183758_1216709448","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:88"},{"text":"%md\n### **Aggregations**\n\nAggregating pertains to collecting something together. In aggregations, you will specify a key aor grouping that indicate how to transform one or more columns. We will review two type of aggregations: `groupby` and `window`. Let's load our test data:\n```python\ndf = spark.read.format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"inferSchema\", \"true\")\\\n    .load(\"/data/retail-data/all/*.csv\")\\\n    .coalesce(5)\ndf.cache()\n\ndf.createOrReplaceTempView(\"dfTable\")\n```\n","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3><strong>Aggregations</strong></h3>\n<p>Aggregating pertains to collecting something together. In aggregations, you will specify a key aor grouping that indicate how to transform one or more columns. We will review two type of aggregations: <code>groupby</code> and <code>window</code>. Let&rsquo;s load our test data:</p>\n<pre><code class=\"python\">df = spark.read.format(&quot;csv&quot;)\\\n    .option(&quot;header&quot;, &quot;true&quot;)\\\n    .option(&quot;inferSchema&quot;, &quot;true&quot;)\\\n    .load(&quot;/data/retail-data/all/*.csv&quot;)\\\n    .coalesce(5)\ndf.cache()\n\ndf.createOrReplaceTempView(&quot;dfTable&quot;)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615014_215943660","id":"20210120-184204_1808293040","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:89"},{"text":"%pyspark\n\ndf = sq.read.format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"inferSchema\", \"true\")\\\n    .load(path_to_retail_data)\\\n    .coalesce(5)\n    \ndf.cache()\n\n\n\n\n\n\n\n\n\n\ndf.createOrReplaceTempView(\"dfTable\")","user":"innocent","dateUpdated":"2021-01-21T12:29:57+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1611215615015_-1402425547","id":"20210120-184240_1478282031","dateCreated":"2021-01-21T07:53:35+0000","dateStarted":"2021-01-21T12:29:57+0000","dateFinished":"2021-01-21T12:30:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90"},{"text":"%md\n### 1. Some Aggregating Functions\n#### **count**\n```python\nfrom pyspark.sql.functions import count\ndf.select(count(\"StockCode\")).show() # 541909\n```\n\n#### **countDistinct**\n*Getting to total number of unique groups*\n```python\nfrom pyspark.sql.functions import count\ndf.select(countDistinct(\"StockCode\")).show() # 4070\n```\n#### **sum ans average**\n```python\ndf.select(\n    count(\"Quantity\").alias(\"total_transactions\"),\n    sum(\"Quantity\").alias(\"total_purchases\"),\n    avg(\"Quantity\").alias(\"avg_purchases\"),\n    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n    .selectExpr(\n        \"total_purchases/total_transactions\",\n        \"avg_purchases\",\n        \"mean_purchases\").show()\n```","user":"innocent","dateUpdated":"2021-01-21T12:29:48+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>1. Some Aggregating Functions</h3>\n<h4><strong>count</strong></h4>\n<pre><code class=\"python\">from pyspark.sql.functions import count\ndf.select(count(&quot;StockCode&quot;)).show() # 541909\n</code></pre>\n<h4><strong>countDistinct</strong></h4>\n<p><em>Getting to total number of unique groups</em></p>\n<pre><code class=\"python\">from pyspark.sql.functions import count\ndf.select(countDistinct(&quot;StockCode&quot;)).show() # 4070\n</code></pre>\n<h4><strong>sum ans average</strong></h4>\n<pre><code class=\"python\">df.select(\n    count(&quot;Quantity&quot;).alias(&quot;total_transactions&quot;),\n    sum(&quot;Quantity&quot;).alias(&quot;total_purchases&quot;),\n    avg(&quot;Quantity&quot;).alias(&quot;avg_purchases&quot;),\n    expr(&quot;mean(Quantity)&quot;).alias(&quot;mean_purchases&quot;))\\\n    .selectExpr(\n        &quot;total_purchases/total_transactions&quot;,\n        &quot;avg_purchases&quot;,\n        &quot;mean_purchases&quot;).show()\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615015_1301893609","id":"20210120-184639_410316119","dateCreated":"2021-01-21T07:53:35+0000","dateStarted":"2021-01-21T12:29:48+0000","dateFinished":"2021-01-21T12:29:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:91"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import count, countDistinct, sum, expr, avg\n# count\ndf.select(count(\"StockCode\")).show() \n\n# countDistinct\ndf.select(countDistinct(\"StockCode\")).show()\n\n# sum and average\ndf.select(\n    count(\"Quantity\").alias(\"total_transactions\"),\n    sum(\"Quantity\").alias(\"total_purchases\"),\n    avg(\"Quantity\").alias(\"avg_purchases\"),\n    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n    .selectExpr(\n        \"total_purchases/total_transactions\",\n        \"avg_purchases\",\n        \"mean_purchases\").show()","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------+\n|count(StockCode)|\n+----------------+\n|          541909|\n+----------------+\n\n+-------------------------+\n|count(DISTINCT StockCode)|\n+-------------------------+\n|                     4070|\n+-------------------------+\n\n+--------------------------------------+----------------+----------------+\n|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n+--------------------------------------+----------------+----------------+\n|                      9.55224954743324|9.55224954743324|9.55224954743324|\n+--------------------------------------+----------------+----------------+\n\n"}]},"apps":[],"jobName":"paragraph_1611215615016_2118976849","id":"20210120-184714_543595074","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:92"},{"text":"%md\n### 2. Grouping\nWe can also perform some calculation in groups of data.\n```python\nfrom pyspark.sql.functions import count\n\ndf.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()\n\ndf.groupBy(\"InvoiceNo\").agg(\n    count(\"Quantity\").alias(\"quan\"),\n    expr(\"count(Quantity)\")).show()\n```\n","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>2. Grouping</h3>\n<p>We can also perform some calculation in groups of data.</p>\n<pre><code class=\"python\">from pyspark.sql.functions import count\n\ndf.groupBy(&quot;InvoiceNo&quot;, &quot;CustomerId&quot;).count().show()\n\ndf.groupBy(&quot;InvoiceNo&quot;).agg(\n    count(&quot;Quantity&quot;).alias(&quot;quan&quot;),\n    expr(&quot;count(Quantity)&quot;)).show()\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615017_-729020124","id":"20210120-185044_1360642616","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:93"},{"text":"%pyspark\ndf.show()","user":"innocent","dateUpdated":"2021-01-21T12:32:28+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|\n|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|\n|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|\n|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|\n|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|\n|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|\n|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|\n|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1611232327609_-1875195916","id":"20210121-123207_1054780184","dateCreated":"2021-01-21T12:32:07+0000","dateStarted":"2021-01-21T12:32:16+0000","dateFinished":"2021-01-21T12:34:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:94"},{"text":"%pyspark\n\nin_cus = df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()\n\nin_qu = df.groupBy(\"InvoiceNo\").agg(\n    count(\"Quantity\").alias(\"quan\"),\n    expr(\"count(Quantity)\")).show()\n\nin_cus, in_qu","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+----------+-----+\n|InvoiceNo|CustomerId|count|\n+---------+----------+-----+\n|   536846|     14573|   76|\n|   537026|     12395|   12|\n|   537883|     14437|    5|\n|   538068|     17978|   12|\n|   538279|     14952|    7|\n|   538800|     16458|   10|\n|   538942|     17346|   12|\n|  C539947|     13854|    1|\n|   540096|     13253|   16|\n|   540530|     14755|   27|\n|   541225|     14099|   19|\n|   541978|     13551|    4|\n|   542093|     17677|   16|\n|   543188|     12567|   63|\n|   543590|     17377|   19|\n|  C543757|     13115|    1|\n|  C544318|     12989|    1|\n|   544578|     12365|    1|\n|   545165|     16339|   20|\n|   545289|     14732|   30|\n+---------+----------+-----+\nonly showing top 20 rows\n\n+---------+----+---------------+\n|InvoiceNo|quan|count(Quantity)|\n+---------+----+---------------+\n|   536596|   6|              6|\n|   536938|  14|             14|\n|   537252|   1|              1|\n|   537691|  20|             20|\n|   538041|   1|              1|\n|   538184|  26|             26|\n|   538517|  53|             53|\n|   538879|  19|             19|\n|   539275|   6|              6|\n|   539630|  12|             12|\n|   540499|  24|             24|\n|   540540|  22|             22|\n|  C540850|   1|              1|\n|   540976|  48|             48|\n|   541432|   4|              4|\n|   541518| 101|            101|\n|   541783|  35|             35|\n|   542026|   9|              9|\n|   542375|   6|              6|\n|  C542604|   8|              8|\n+---------+----+---------------+\nonly showing top 20 rows\n\n(None, None)\n"}]},"apps":[],"jobName":"paragraph_1611215615017_390890991","id":"20210120-185048_91218857","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95"},{"text":"%md\n### 3. Window Functions\nWhen `groupby` takes data, every row can go only into one grouping. A window function calculates a return value for every input row of a table based on a group of rows. We will perform daily rolling averages on our data to demonstrate windows. First, lets add a new column to our Dataframe. \n\n```python\nfrom pyspark.sql.functions import col, to_date\ndfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\ndfWithDate.createOrReplaceTempView(\"dfWithDate\")\n```\nThe first step is to **create a window specification**. Our spec is:\n```python \nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import desc\nwindowSpec = Window\\\n.partitionBy(\"CustomerId\", \"date\")\\ # This is the window frame we will be operating on\n.orderBy(desc(\"Quantity\"))\\         # This sorts within our frame\n.rowsBetween(Window.unboundedPreceding, Window.currentRow) # This get the current row and every row preceding it. \n```\n\nWith out spec, we can now use a window function. In this case we will be using an aggregation function.\n```python\nfrom pyspark.sql.functions import max\nmaxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec) # GEt the max quantity in the window and assign to respective rows. \n```\n\nWe can use this expression in a Dataframe `select` statement. Before we show the result we also want to know the purchase quantity rank. We use the `dense_rank` function to determine which data has the maximum pruchase quantity for every customer. \n```python\n# in Python\nfrom pyspark.sql.functions import dense_rank, rank\npurchaseDenseRank = dense_rank().over(windowSpec)\npurchaseRank = rank().over(windowSpec)\n```\nWe can now call our select statment:\n```python\nfrom pyspark.sql.functions import col\ndfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n    .select(\n     col(\"CustomerId\"),\n     col(\"date\"),\n     col(\"Quantity\"),\n     purchaseRank.alias(\"quantityRank\"),\n     purchaseDenseRank.alias(\"quantityDenseRank\"),\n     maxPurchaseQuantity.alias(\"maxPurchaseQuantity\"))\n    .show()\n```","user":"innocent","dateUpdated":"2021-01-21T12:38:06+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>3. Window Functions</h3>\n<p>When <code>groupby</code> takes data, every row can go only into one grouping. A window function calculates a return value for every input row of a table based on a group of rows. We will perform daily rolling averages on our data to demonstrate windows. First, lets add a new column to our Dataframe. </p>\n<pre><code class=\"python\">from pyspark.sql.functions import col, to_date\ndfWithDate = df.withColumn(&quot;date&quot;, to_date(col(&quot;InvoiceDate&quot;), &quot;MM/d/yyyy H:mm&quot;))\ndfWithDate.createOrReplaceTempView(&quot;dfWithDate&quot;)\n</code></pre>\n<p>The first step is to <strong>create a window specification</strong>. Our spec is:</p>\n<pre><code class=\"python \">from pyspark.sql.window import Window\nfrom pyspark.sql.functions import desc\nwindowSpec = Window\\\n.partitionBy(&quot;CustomerId&quot;, &quot;date&quot;)\\ # This is the window frame we will be operating on\n.orderBy(desc(&quot;Quantity&quot;))\\         # This sorts within our frame\n.rowsBetween(Window.unboundedPreceding, Window.currentRow) # This get the current row and every row preceding it. \n</code></pre>\n<p>With out spec, we can now use a window function. In this case we will be using an aggregation function.</p>\n<pre><code class=\"python\">from pyspark.sql.functions import max\nmaxPurchaseQuantity = max(col(&quot;Quantity&quot;)).over(windowSpec) # GEt the max quantity in the window and assign to respective rows. \n</code></pre>\n<p>We can use this expression in a Dataframe <code>select</code> statement. Before we show the result we also want to know the purchase quantity rank. We use the <code>dense_rank</code> function to determine which data has the maximum pruchase quantity for every customer. </p>\n<pre><code class=\"python\"># in Python\nfrom pyspark.sql.functions import dense_rank, rank\npurchaseDenseRank = dense_rank().over(windowSpec)\npurchaseRank = rank().over(windowSpec)\n</code></pre>\n<p>We can now call our select statment:</p>\n<pre><code class=\"python\">from pyspark.sql.functions import col\ndfWithDate.where(&quot;CustomerId IS NOT NULL&quot;).orderBy(&quot;CustomerId&quot;)\\\n    .select(\n     col(&quot;CustomerId&quot;),\n     col(&quot;date&quot;),\n     col(&quot;Quantity&quot;),\n     purchaseRank.alias(&quot;quantityRank&quot;),\n     purchaseDenseRank.alias(&quot;quantityDenseRank&quot;),\n     maxPurchaseQuantity.alias(&quot;maxPurchaseQuantity&quot;))\n    .show()\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615018_-1790307104","id":"20201125-203717_1510238154","dateCreated":"2021-01-21T07:53:35+0000","dateStarted":"2021-01-21T12:38:06+0000","dateFinished":"2021-01-21T12:38:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:96"},{"text":"%pyspark\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import desc, max, dense_rank, rank, col\n\ndfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\ndfWithDate.createOrReplaceTempView(\"dfWithDate\")\n\n# Create a window spec\nwindowSpec = Window\\\n    .partitionBy(\"CustomerId\", \"date\")\\\n    .orderBy(desc(\"Quantity\"))\\\n    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\nmaxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)\npurchaseDenseRank = dense_rank().over(windowSpec)\npurchaseRank = rank().over(windowSpec)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n    .select(\n     col(\"CustomerId\"),\n     col(\"date\"),\n     col(\"Quantity\"),\n     purchaseRank.alias(\"quantityRank\"),\n     purchaseDenseRank.alias(\"quantityDenseRank\"),\n     maxPurchaseQuantity.alias(\"maxPurchaseQuantity\"))\\\n    .show()\n\n","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+----------+--------+------------+-----------------+-------------------+\n|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n+----------+----------+--------+------------+-----------------+-------------------+\n|     12346|2011-01-18|   74215|           1|                1|              74215|\n|     12346|2011-01-18|  -74215|           2|                2|              74215|\n|     12347|2010-12-07|      36|           1|                1|                 36|\n|     12347|2010-12-07|      30|           2|                2|                 36|\n|     12347|2010-12-07|      24|           3|                3|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|       6|          17|                5|                 36|\n|     12347|2010-12-07|       6|          17|                5|                 36|\n+----------+----------+--------+------------+-----------------+-------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1611215615019_-1265793839","id":"20210120-185235_1712586493","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:97"},{"text":"%md\n### **Joins**\nWe are all generally familiar with joins. A *join* brings together two set of data, left and right, by comparing the value of one or more keys of the left and rights. Here are the types of joins:\n\n* Inner joins (keep rows with keys that exist in the left and right datasets)\n* Outer joins (keep rows with keys in either the left or right datasets)\n* Left outer joins (keep rows with keys in the left dataset)\n* Right outer joins (keep rows with keys in the right dataset)\n* Left semi joins (keep the rows in the left, and only the left, dataset where the key appears in the right dataset)\n* Left anti joins (keep the rows in the left, and only the left, dataset where they do not appear in the right dataset)\n* Natural joins (perform a join by implicitly matching the columns between the two datasets with the same names)\n* Cross (or Cartesian) joins (match every row in the left dataset with every row in the right dataset)\n\nTo get started lets create some tables and register them:\n```pyhton\nperson = spark.createDataFrame([\n    (0, \"Bill Chambers\", 0, [100]),\n    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n    .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n\ngraduateProgram = spark.createDataFrame([\n    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n    .toDF(\"id\", \"degree\", \"department\", \"school\")\n\nsparkStatus = spark.createDataFrame([\n    (500, \"Vice President\"),\n    (250, \"PMC Member\"),\n    (100, \"Contributor\")])\\\n    .toDF(\"id\", \"status\")\n\nperson.createOrReplaceTempView(\"person\")\ngraduateProgram.createOrReplaceTempView(\"graduateProgram\")\nsparkStatus.createOrReplaceTempView(\"sparkStatus\")\n```","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3><strong>Joins</strong></h3>\n<p>We are all generally familiar with joins. A <em>join</em> bring together tow set of data, left and right, by comparing the value of one or more keys of the left and rights. Here are the types of joins:</p>\n<ul>\n  <li>Inner joins (keep rows with keys that exist in the left and right datasets)</li>\n  <li>Outer joins (keep rows with keys in either the left or right datasets)</li>\n  <li>Left outer joins (keep rows with keys in the left dataset)</li>\n  <li>Right outer joins (keep rows with keys in the right dataset)</li>\n  <li>Left semi joins (keep the rows in the left, and only the left, dataset where the key appears in the right dataset)</li>\n  <li>Left anti joins (keep the rows in the left, and only the left, dataset where they do not appear in the right dataset)</li>\n  <li>Natural joins (perform a join by implicitly matching the columns between the two datasets with the same names)</li>\n  <li>Cross (or Cartesian) joins (match every row in the left dataset with every row in the right dataset)</li>\n</ul>\n<p>To get started lets create some tables and register them:</p>\n<pre><code class=\"pyhton\">person = spark.createDataFrame([\n    (0, &quot;Bill Chambers&quot;, 0, [100]),\n    (1, &quot;Matei Zaharia&quot;, 1, [500, 250, 100]),\n    (2, &quot;Michael Armbrust&quot;, 1, [250, 100])])\\\n    .toDF(&quot;id&quot;, &quot;name&quot;, &quot;graduate_program&quot;, &quot;spark_status&quot;)\n\ngraduateProgram = spark.createDataFrame([\n    (0, &quot;Masters&quot;, &quot;School of Information&quot;, &quot;UC Berkeley&quot;),\n    (2, &quot;Masters&quot;, &quot;EECS&quot;, &quot;UC Berkeley&quot;),\n    (1, &quot;Ph.D.&quot;, &quot;EECS&quot;, &quot;UC Berkeley&quot;)])\\\n    .toDF(&quot;id&quot;, &quot;degree&quot;, &quot;department&quot;, &quot;school&quot;)\n\nsparkStatus = spark.createDataFrame([\n    (500, &quot;Vice President&quot;),\n    (250, &quot;PMC Member&quot;),\n    (100, &quot;Contributor&quot;)])\\\n    .toDF(&quot;id&quot;, &quot;status&quot;)\n\nperson.createOrReplaceTempView(&quot;person&quot;)\ngraduateProgram.createOrReplaceTempView(&quot;graduateProgram&quot;)\nsparkStatus.createOrReplaceTempView(&quot;sparkStatus&quot;)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615020_-910169205","id":"20210120-190148_1806453915","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:98"},{"text":"%pyspark\n\n\"\"\"\nWe are all generally familiar with joins. A join bring together tow set of data, left and right, by comparing the value of one or more keys of the left and rights. Here are the types of joins:\n\nInner joins (keep rows with keys that exist in the left and right datasets)\nOuter joins (keep rows with keys in either the left or right datasets)\nLeft outer joins (keep rows with keys in the left dataset)\nRight outer joins (keep rows with keys in the right dataset)\nLeft semi joins (keep the rows in the left, and only the left, dataset where the key appears in the right dataset)\nLeft anti joins (keep the rows in the left, and only the left, dataset where they do not appear in the right dataset)\nNatural joins (perform a join by implicitly matching the columns between the two datasets with the same names)\nCross (or Cartesian) joins (match every row in the left dataset with every row in the right dataset)\nTo get started lets create some tables and register th\n\"\"\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperson = sq.createDataFrame([\n    (0, \"Bill Chambers\", 0, [100]),\n    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n    .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n\ngraduateProgram = sq.createDataFrame([\n    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n    .toDF(\"id\", \"degree\", \"department\", \"school\")\n\nsparkStatus = sq.createDataFrame([\n    (500, \"Vice President\"),\n    (250, \"PMC Member\"),\n    (100, \"Contributor\")])\\\n    .toDF(\"id\", \"status\")\n\nperson.createOrReplaceTempView(\"person\")\ngraduateProgram.createOrReplaceTempView(\"graduateProgram\")\nsparkStatus.createOrReplaceTempView(\"sparkStatus\")","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1611215615020_1933225572","id":"20210120-190207_558849684","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:99"},{"text":"%md\nWe will review just two types of join: `inner` and `outer`. First we will write out our join expressions\n### A. Inner Join\n```python\ninnerJoinExpression = person[\"graduate_program\"] == graduateProgram['id']\n\n## Now we can run our `join`:\nperson.join(graduateProgram, joinExpression).show()\n\n## To specify the type we care a `joinType` parameter:\njoinType = \"inner\"\nperson.join(graduateProgram, joinExpression, joinType).show()\n```\n### B. Outer Join\n```python\njoinType = \"outer\"\nperson.join(graduateProgram, joinExpression, joinType).show()\n```\nThis is equivalent to writing the following in SQL:\n```sql\nSELECT * FROM person FULL OUTER JOIN graduateProgram \n    ON graduate_program = graduateProgram.id\n```\n\n","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We will review just two types of join: <code>inner</code> and <code>outer</code>. First we will write out our join expressions</p>\n<h3>A. Inner Join</h3>\n<pre><code class=\"python\">innerJoinExpression = person[&quot;graduate_program&quot;] == graduateProgram[&#39;id&#39;]\n\n## Now we can run our `join`:\nperson.join(graduateProgram, joinExpression).show()\n\n## To specify the type we care a `joinType` parameter:\njoinType = &quot;inner&quot;\nperson.join(graduateProgram, joinExpression, joinType).show()\n</code></pre>\n<h3>B. Outer Join</h3>\n<pre><code class=\"python\">joinType = &quot;outer&quot;\nperson.join(graduateProgram, joinExpression, joinType).show()\n</code></pre>\n<p>This is equivalent to writing the following in SQL:</p>\n<pre><code class=\"sql\">SELECT * FROM person FULL OUTER JOIN graduateProgram \n    ON graduate_program = graduateProgram.id\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615021_-1274756418","id":"20201125-203915_1437852763","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:100"},{"text":"%pyspark\n# Inner\njoinExpression = person[\"graduate_program\"] == graduateProgram['id']\n\n## Now we can run our `join`:\nperson.join(graduateProgram, joinExpression).show()\n\n## To specify the type we care a `joinType` parameter:\njoinType = \"inner\"\nperson.join(graduateProgram, joinExpression, joinType).show()\n\n\n# Outer\njoinType = \"outer\"\nperson.join(graduateProgram, joinExpression, joinType).show()\n\nspark.sql(\"\"\"SELECT * FROM person FULL OUTER JOIN graduateProgram \n    ON graduate_program = graduateProgram.id\"\"\").show()","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n"}]},"apps":[],"jobName":"paragraph_1611215615022_900572509","id":"20210120-190344_470783828","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:101"},{"text":"%md\n### **Reading and Writing Files**\n#### A. Read Data\nWhen reading data we specify:\n* The *format*\n* The *schema*\n* The *read mode*\n* A series of *options*\n\nA read looks like this:\n```python\nspark.read.format(\"csv\")\\\n    .option(\"mode\", \"FAILFAST\")\\\n    .option(\"inferSchema\", \"true\")\\\n    .option(\"path\", \"path/to/file(s)\")\\\n    .schema(someSchema)\n    .load()\n```\n\n#### B. Write Data\nWhen writing data we specify:\n* The *format*\n* The *schema*\n* The *read mode*\n* A series of *options*\n\nA write looks like this:\n```python\ndataframe.write.format(\"csv\")\\\n    .option(\"mode\", \"overwrite\")\\ # overwrite any data at destination that matches path\n    .option(\"dateFormat\", \"yyyy-MM-dd\")\\\n    .option(\"header\", \"true\")\\ # include the head in each csv file\n    .option(\"path\", \"path/to/file(s)\")\n    .save()\n```","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3><strong>Reading and Writing Files</strong></h3>\n<h4>A. Read Data</h4>\n<p>When reading data we specify:<br/>* The <em>format</em><br/>* The <em>schema</em><br/>* The <em>read mode</em><br/>* A series of <em>options</em></p>\n<p>A read looks like this:</p>\n<pre><code class=\"python\">spark.read.format(&quot;csv&quot;)\\\n    .option(&quot;mode&quot;, &quot;FAILFAST&quot;)\\\n    .option(&quot;inferSchema&quot;, &quot;true&quot;)\\\n    .option(&quot;path&quot;, &quot;path/to/file(s)&quot;)\\\n    .schema(someSchema)\n    .load()\n</code></pre>\n<h4>B. Write Data</h4>\n<p>When writing data we specify:<br/>* The <em>format</em><br/>* The <em>schema</em><br/>* The <em>read mode</em><br/>* A series of <em>options</em></p>\n<p>A write looks like this:</p>\n<pre><code class=\"python\">dataframe.write.format(&quot;csv&quot;)\\\n    .option(&quot;mode&quot;, &quot;overwrite&quot;)\\ # overwrite any data at destination that matches path\n    .option(&quot;dateFormat&quot;, &quot;yyyy-MM-dd&quot;)\\\n    .option(&quot;header&quot;, &quot;true&quot;)\\ # include the head in each csv file\n    .option(&quot;path&quot;, &quot;path/to/file(s)&quot;)\n    .save()\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615022_-1589328614","id":"20201125-203932_471544402","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:102"},{"text":"%md\n#### C. Worked CSV Example\n```python\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType\n\n## Reading\nmyManualSchema = StructType([\n    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n    StructField(\"count\", LongType(), False)\n))\n\nspark.read.format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"mode\", \"FAILFAST\")\\\n    .schema(myManualSchema)\\\n    .load(\"/data/flight-data/csv/2010-summary.csv\")\\\n    .show(5)\n\n## Writing\ncsvFile = spark.write.format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"mode\", \"OVERWRITE\")\\\n    .option(\"inferSchema\", \"true\")\\\n    .load(file_path)\n```\n","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>C. Worked CSV Example</h4>\n<pre><code class=\"python\">from pyspark.sql.types import StructField, StructType, StringType, LongType\n\n## Reading\nmyManualSchema = StructType([\n    StructField(&quot;DEST_COUNTRY_NAME&quot;, StringType(), True),\n    StructField(&quot;ORIGIN_COUNTRY_NAME&quot;, StringType(), True),\n    StructField(&quot;count&quot;, LongType(), False)\n))\n\nspark.read.format(&quot;csv&quot;)\\\n    .option(&quot;header&quot;, &quot;true&quot;)\\\n    .option(&quot;mode&quot;, &quot;FAILFAST&quot;)\\\n    .schema(myManualSchema)\\\n    .load(&quot;/data/flight-data/csv/2010-summary.csv&quot;)\\\n    .show(5)\n\n## Writing\ncsvFile = spark.write.format(&quot;csv&quot;)\\\n    .option(&quot;header&quot;, &quot;true&quot;)\\\n    .option(&quot;mode&quot;, &quot;OVERWRITE&quot;)\\\n    .option(&quot;inferSchema&quot;, &quot;true&quot;)\\\n    .load(file_path)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1611215615023_-1015841271","id":"20201125-203949_827643005","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103"},{"text":"%pyspark\n\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType\n\nname = ''\nfile_path = \"s3n://sierra-leone-lake/training/data/{}-demo.csv\".format(name)\n## Reading\nmyManualSchema = StructType([\n    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n    StructField(\"count\", LongType(), False)\n    ])\n\nsq.read.format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"mode\", \"FAILFAST\")\\\n    .schema(myManualSchema)\\\n    .load(\"s3n://sierra-leone-lake/training/data/demo/flight-data/csv/2010-summary.csv\")\\\n    .show(5)\n\n## Writing\n\"\"\"csvFile = sq.write.format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"mode\", \"OVERWRITE\")\\\n    .option(\"inferSchema\", \"true\")\\\n    .load(file_path)\n    \"\"\"","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n'csvFile = sq.write.format(\"csv\")    .option(\"header\", \"true\")    .option(\"mode\", \"OVERWRITE\")    .option(\"inferSchema\", \"true\")    .load(file_path)\\n    '\n"}]},"apps":[],"jobName":"paragraph_1611215615024_-2001831205","id":"20210120-190824_1624267554","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:104"},{"text":"%md\n","user":"innocent","dateUpdated":"2021-01-21T07:53:35+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1611215615025_222468621","id":"20210120-190723_97544723","dateCreated":"2021-01-21T07:53:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:105"}],"name":"Training/0.Intro-to-Spark-v2","id":"2FV5HHWCM","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}